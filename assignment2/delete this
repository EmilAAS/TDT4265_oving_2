l = [1,1,1,1,1,1,1]
for i in range(len(l)-1,-1,-1):
    print(i)




















def backward(self, X: np.ndarray, outputs: np.ndarray, targets: np.ndarray) -> None:
    """
    Computes the gradient and saves it to the variable self.grad

    Args:
        X: images of shape [batch size, 785]
        outputs: outputs of model of shape: [batch size, num_outputs]
        targets: labels/targets of each image of shape: [batch size, num_classes]
    """
    assert (
        targets.shape == outputs.shape
    ), f"Output shape: {outputs.shape}, targets: {targets.shape}"

    # Initialize a list to store gradients for each layer
    self.grads = []

    # Compute the gradient for the output layer
    delta_k = -(targets - outputs)
    self.grads.append(self.hidden_layer_output[-1].T.dot(delta_k) / X.shape[0])

    # Backpropagate the error through the hidden layers
    for layer_index in range(self.num_hidden_layers - 1, 0, -1):
        delta_j = (
            sigmoid_derivative(self.hidden_layer_output[layer_index], self.use_improved_sigmoid)
            * delta_k.dot(self.ws[layer_index].T)
        )
        self.grads.insert(0, self.hidden_layer_output[layer_index - 1].T.dot(delta_j) / X.shape[0])

    # Compute the gradient for the first hidden layer
    delta_j_first = sigmoid_derivative(self.hidden_layer_output[0], self.use_improved_sigmoid) * delta_k.dot(
        self.ws[0].T
    )
    self.grads.insert(0, X.T.dot(delta_j_first) / X.shape[0])

    # Check the shape of the gradients against the corresponding weights
    for grad, w in zip(self.grads, self.ws):
        assert grad.shape == w.shape, f"Expected the same shape. Grad shape: {grad.shape}, w: {w.shape}."
